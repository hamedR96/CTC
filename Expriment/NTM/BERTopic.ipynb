{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import swifter\n",
    "from bertopic import BERTopic\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[{}0-9]'.format(string.punctuation), ' ', text)\n",
    "    text=re.sub(r'[^A-Za-z0-9 ]+', ' ', text)\n",
    "    text = word_tokenize(text)\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    text = [WordNetLemmatizer().lemmatize(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/18846 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7014d3eb510344098f48bba630d6da2c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import swifter\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "df=pd.DataFrame({\"content\":newsgroups[\"data\"]})\n",
    "\n",
    "#df=df.sample(1000) #for a sample of 1000 documents\n",
    "df[\"content\"]=df[\"content\"].swifter.apply(lambda x: preprocess_text(x))\n",
    "df['content_length'] = df['content'].str.len()\n",
    "\n",
    "df = df[df['content_length'] > 100]\n",
    "df = df[df['content_length'] < 2000]\n",
    "\n",
    "df=df[[\"content\"]].reset_index(drop=True).reset_index().rename(columns={\"index\":\"id\"})\n",
    "documents=df.content.to_list()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(documents)\n",
    "topic_model.save(\"bertopic_20newsgroup\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/14268 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "298d02efb76541f28f50308a4fdec1ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "def count_remover(text,threshold=4):\n",
    "    if len(text.split())<threshold:\n",
    "        return pd.NaT\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "data=pd.read_json('tweets.json' ,lines=True)\n",
    "df=data[[\"Text\",\"CreatedAt\"]].rename(columns={\"Text\":\"content\",\"CreatedAt\":\"time\"})\n",
    "df['content'] = df['content'].str.replace(r'@\\w+', '')\n",
    "df['content'] = df['content'].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
    "df['content'] = df['content'].apply(lambda x: remove_punct(x))\n",
    "df['content'] = df['content'].apply(lambda x: count_remover(x))\n",
    "df=df.dropna()\n",
    "df[\"content\"]=df[\"content\"].swifter.apply(lambda x: preprocess_text(x))\n",
    "df=df.dropna()\n",
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(df.content.tolist())\n",
    "#topic_model.save(\"bertopic_EM_tweets\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "topic_model.get_topics()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic.load(\"results/bertopic_20newsgroup\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217\n"
     ]
    }
   ],
   "source": [
    "topics=[]\n",
    "t=topic_model.get_topics()\n",
    "for i in range(len(t)-1):\n",
    "    topic=[]\n",
    "    for w in t[i]:\n",
    "        topic.append(w[0])\n",
    "    topics.append(topic)\n",
    "print(len(topics))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"results/bertopic_EM_tweets_topics\", \"wb\") as fp:   #Pickling\n",
    "     pickle.dump(topics, fp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "document_topic_distribution=topic_model.transform(documents)[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "documents_distribution=[]\n",
    "for i,document in enumerate(document_topic_distribution):\n",
    "    document_dist=np.zeros(len(topics))\n",
    "    if document>-1:\n",
    "        document_dist[document]=1\n",
    "    documents_distribution.append(document_dist)\n",
    "documents_topic_distribution_array = np.vstack(documents_distribution)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "\n",
    "with open(\"results/bertopic_EM_tweets_top_doc_dist\", \"wb\") as fp:   #Pickling\n",
    "     pickle.dump(documents_topic_distribution_array, fp)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
