{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import swifter\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[{}0-9]'.format(string.punctuation), ' ', text)\n",
    "    text=re.sub(r'[^A-Za-z0-9 ]+', ' ', text)\n",
    "    text = word_tokenize(text)\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    text = [WordNetLemmatizer().lemmatize(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/18846 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8d82db059bd4faa9a8a4ebad4ca6d1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "df=pd.DataFrame({\"content\":newsgroups[\"data\"]})\n",
    "\n",
    "df[\"content\"]=df[\"content\"].swifter.apply(lambda x: preprocess_text(x))\n",
    "df['content_length'] = df['content'].str.len()\n",
    "\n",
    "df = df[df['content_length'] > 100]\n",
    "df = df[df['content_length'] < 2000]\n",
    "\n",
    "df=df[[\"content\"]].reset_index(drop=True).reset_index().rename(columns={\"index\":\"id\"})\n",
    "documents_20newsgroup=df.content.to_list()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/14268 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b88ff39955e84f86a6f43d860481aabd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "def count_remover(text,threshold=4):\n",
    "    if len(text.split())<threshold:\n",
    "        return pd.NaT\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "data=pd.read_json('tweets.json' ,lines=True)\n",
    "df=data[[\"Text\",\"CreatedAt\"]].rename(columns={\"Text\":\"content\",\"CreatedAt\":\"time\"})\n",
    "df['content'] = df['content'].str.replace(r'@\\w+', '')\n",
    "df['content'] = df['content'].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
    "df['content'] = df['content'].apply(lambda x: remove_punct(x))\n",
    "df['content'] = df['content'].apply(lambda x: count_remover(x))\n",
    "df=df.dropna()\n",
    "df[\"content\"]=df[\"content\"].swifter.apply(lambda x: preprocess_text(x))\n",
    "df=df.dropna()\n",
    "documents_EM_tweets=df.content.tolist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import time\n",
    "import jieba\n",
    "\n",
    "def preprocessing(documents):\n",
    "\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    docs = []\n",
    "    currentDocument = []\n",
    "    currentWordId = 0\n",
    "\n",
    "    for document in documents:\n",
    "        segList = jieba.cut(document)\n",
    "        for word in segList:\n",
    "            word = word.lower().strip()\n",
    "            # 单词长度大于1并且不包含数字并且不是停止词\n",
    "            if len(word) > 1 and not re.search('[0-9]', word) and word not in stop_words:\n",
    "                if word in word2id:\n",
    "                    currentDocument.append(word2id[word])\n",
    "                else:\n",
    "                    currentDocument.append(currentWordId)\n",
    "                    word2id[word] = currentWordId\n",
    "                    id2word[currentWordId] = word\n",
    "                    currentWordId += 1\n",
    "        docs.append(currentDocument)\n",
    "        currentDocument = []\n",
    "    return docs, word2id, id2word\n",
    "\n",
    "# 初始化，按照每个topic概率都相等的multinomial分布采样，等价于取随机数，并更新采样出的topic的相关计数\n",
    "def randomInitialize():\n",
    "\tfor d, doc in enumerate(docs):\n",
    "\t\tzCurrentDoc = []\n",
    "\t\tfor w in doc:\n",
    "\t\t\tpz = np.divide(np.multiply(ndz[d, :], nzw[:, w]), nz)\n",
    "\t\t\tz = np.random.multinomial(1, pz / pz.sum()).argmax()\n",
    "\t\t\tzCurrentDoc.append(z)\n",
    "\t\t\tndz[d, z] += 1\n",
    "\t\t\tnzw[z, w] += 1\n",
    "\t\t\tnz[z] += 1\n",
    "\t\tZ.append(zCurrentDoc)\n",
    "\n",
    "# gibbs采样\n",
    "def gibbsSampling():\n",
    "\t# 为每个文档中的每个单词重新采样topic\n",
    "\tfor d, doc in enumerate(docs):\n",
    "\t\tfor index, w in enumerate(doc):\n",
    "\t\t\tz = Z[d][index]\n",
    "\t\t\t# 将当前文档当前单词原topic相关计数减去1\n",
    "\t\t\tndz[d, z] -= 1\n",
    "\t\t\tnzw[z, w] -= 1\n",
    "\t\t\tnz[z] -= 1\n",
    "\t\t\t# 重新计算当前文档当前单词属于每个topic的概率\n",
    "\t\t\tpz = np.divide(np.multiply(ndz[d, :], nzw[:, w]), nz)\n",
    "\t\t\t# 按照计算出的分布进行采样\n",
    "\t\t\tz = np.random.multinomial(1, pz / pz.sum()).argmax()\n",
    "\t\t\tZ[d][index] = z\n",
    "\t\t\t# 将当前文档当前单词新采样的topic相关计数加上1\n",
    "\t\t\tndz[d, z] += 1\n",
    "\t\t\tnzw[z, w] += 1\n",
    "\t\t\tnz[z] += 1\n",
    "\n",
    "def perplexity():\n",
    "\tnd = np.sum(ndz, 1)\n",
    "\tn = 0\n",
    "\tll = 0.0\n",
    "\tfor d, doc in enumerate(docs):\n",
    "\t\tfor w in doc:\n",
    "\t\t\tll = ll + np.log(((nzw[:, w] / nz) * (ndz[d, :] / nd[d])).sum())\n",
    "\t\t\tn = n + 1\n",
    "\treturn np.exp(ll/(-n))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/db/frmp2tzj72s37hdwvvdzwr7m0000gn/T/jieba.cache\n",
      "Loading model cost 0.345 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "docs, word2id, id2word = preprocessing(documents_EM_tweets)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:54:17 Iteration:  0  Completed  Perplexity:  3581.7894045866046\n",
      "10:54:18 Iteration:  1  Completed  Perplexity:  3580.1305484179575\n",
      "10:54:19 Iteration:  2  Completed  Perplexity:  3579.588090697878\n",
      "10:54:20 Iteration:  3  Completed  Perplexity:  3577.695454761753\n",
      "10:54:22 Iteration:  4  Completed  Perplexity:  3577.728074228838\n",
      "10:54:23 Iteration:  5  Completed  Perplexity:  3575.9518054147943\n",
      "10:54:24 Iteration:  6  Completed  Perplexity:  3576.6773707918\n",
      "10:54:25 Iteration:  7  Completed  Perplexity:  3577.083199278167\n",
      "10:54:26 Iteration:  8  Completed  Perplexity:  3576.406269769372\n",
      "10:54:27 Iteration:  9  Completed  Perplexity:  3575.2587023215215\n",
      "10:54:28 Iteration:  10  Completed  Perplexity:  3574.3248961222007\n",
      "10:54:30 Iteration:  11  Completed  Perplexity:  3573.460412861076\n",
      "10:54:31 Iteration:  12  Completed  Perplexity:  3571.7440176716495\n",
      "10:54:32 Iteration:  13  Completed  Perplexity:  3571.743058493773\n",
      "10:54:33 Iteration:  14  Completed  Perplexity:  3572.962261402745\n",
      "10:54:34 Iteration:  15  Completed  Perplexity:  3572.8212203759417\n",
      "10:54:35 Iteration:  16  Completed  Perplexity:  3570.030991445234\n",
      "10:54:36 Iteration:  17  Completed  Perplexity:  3569.29881128618\n",
      "10:54:38 Iteration:  18  Completed  Perplexity:  3568.8059635115533\n",
      "10:54:39 Iteration:  19  Completed  Perplexity:  3567.716943426891\n",
      "10:54:40 Iteration:  20  Completed  Perplexity:  3567.633914688583\n",
      "10:54:41 Iteration:  21  Completed  Perplexity:  3569.23015008214\n",
      "10:54:43 Iteration:  22  Completed  Perplexity:  3567.2089232794647\n",
      "10:54:44 Iteration:  23  Completed  Perplexity:  3567.165662481848\n",
      "10:54:45 Iteration:  24  Completed  Perplexity:  3567.7424094783064\n",
      "10:54:47 Iteration:  25  Completed  Perplexity:  3566.174187531639\n",
      "10:54:48 Iteration:  26  Completed  Perplexity:  3566.9680972519886\n",
      "10:54:49 Iteration:  27  Completed  Perplexity:  3566.947939146224\n",
      "10:54:50 Iteration:  28  Completed  Perplexity:  3565.2351474141283\n",
      "10:54:51 Iteration:  29  Completed  Perplexity:  3562.489065591981\n",
      "10:54:52 Iteration:  30  Completed  Perplexity:  3560.0935813305973\n",
      "10:54:53 Iteration:  31  Completed  Perplexity:  3559.86613647124\n",
      "10:54:54 Iteration:  32  Completed  Perplexity:  3559.8017631463\n",
      "10:54:55 Iteration:  33  Completed  Perplexity:  3557.7380326417815\n",
      "10:54:56 Iteration:  34  Completed  Perplexity:  3556.39149967191\n",
      "10:54:57 Iteration:  35  Completed  Perplexity:  3557.9119603734894\n",
      "10:54:58 Iteration:  36  Completed  Perplexity:  3556.288258208245\n",
      "10:54:59 Iteration:  37  Completed  Perplexity:  3555.5897309560155\n",
      "10:55:01 Iteration:  38  Completed  Perplexity:  3554.3423592285358\n",
      "10:55:02 Iteration:  39  Completed  Perplexity:  3551.158949535033\n",
      "10:55:03 Iteration:  40  Completed  Perplexity:  3548.946208743819\n",
      "10:55:04 Iteration:  41  Completed  Perplexity:  3546.896011795404\n",
      "10:55:05 Iteration:  42  Completed  Perplexity:  3545.64760873626\n",
      "10:55:06 Iteration:  43  Completed  Perplexity:  3545.7454536712116\n",
      "10:55:07 Iteration:  44  Completed  Perplexity:  3546.3115573434475\n",
      "10:55:08 Iteration:  45  Completed  Perplexity:  3546.2427158552946\n",
      "10:55:08 Iteration:  46  Completed  Perplexity:  3543.812310678914\n",
      "10:55:09 Iteration:  47  Completed  Perplexity:  3542.222649447887\n",
      "10:55:10 Iteration:  48  Completed  Perplexity:  3543.290218001933\n",
      "10:55:11 Iteration:  49  Completed  Perplexity:  3542.0335852444823\n"
     ]
    }
   ],
   "source": [
    "alpha = 5\n",
    "beta = 0.1\n",
    "iterationNum = 50\n",
    "Z = []\n",
    "K = 30\n",
    "\n",
    "N = len(docs)\n",
    "M = len(word2id)\n",
    "ndz = np.zeros([N, K]) + alpha\n",
    "nzw = np.zeros([K, M]) + beta\n",
    "nz = np.zeros([K]) + M * beta\n",
    "randomInitialize()\n",
    "for i in range(0, iterationNum):\n",
    "\tgibbsSampling()\n",
    "\tprint(time.strftime('%X'), \"Iteration: \", i, \" Completed\", \" Perplexity: \", perplexity())\n",
    "\n",
    "topicwords = []\n",
    "maxTopicWordsNum = 10\n",
    "for z in range(0, K):\n",
    "\tids = nzw[z, :].argsort()\n",
    "\ttopicword = []\n",
    "\tfor j in ids:\n",
    "\t\ttopicword.insert(0, id2word[j])\n",
    "\ttopicwords.append(topicword[0 : min(10, len(topicword))])\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"results/lda_gibbs_EM_tweets_30_topics\", \"wb\") as fp:   #Pickling\n",
    "     pickle.dump(topicwords, fp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
