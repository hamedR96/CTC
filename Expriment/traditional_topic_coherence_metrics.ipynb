{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import swifter\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[{}0-9]'.format(string.punctuation), ' ', text)\n",
    "    text=re.sub(r'[^A-Za-z0-9 ]+', ' ', text)\n",
    "    text = word_tokenize(text)\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    text = [WordNetLemmatizer().lemmatize(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/14268 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04aff411f7ed4f538b42efaef28e3170"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Read dataset of Elon Musk Tweets\n",
    "\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "def count_remover(text,threshold=4):\n",
    "    if len(text.split())<threshold:\n",
    "        return pd.NaT\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "data=pd.read_json('NTM/tweets.json' ,lines=True)\n",
    "df=data[[\"Text\",\"CreatedAt\"]].rename(columns={\"Text\":\"content\",\"CreatedAt\":\"time\"})\n",
    "df['content'] = df['content'].str.replace(r'@\\w+', '')\n",
    "df['content'] = df['content'].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
    "df['content'] = df['content'].apply(lambda x: remove_punct(x))\n",
    "df['content'] = df['content'].apply(lambda x: count_remover(x))\n",
    "df=df.dropna()\n",
    "df[\"content\"]=df[\"content\"].swifter.apply(lambda x: preprocess_text(x))\n",
    "df=df.dropna()\n",
    "documents_EM_tweets=df.content.tolist()\n",
    "\n",
    "tokenized_docs = [word_tokenize(document.lower()) for document in documents_EM_tweets]\n",
    "model = gensim.models.Word2Vec(tokenized_docs, vector_size=300, window=5, min_count=1, workers=12)\n",
    "wv_EM_tweets=model.wv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/18846 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8edf8ba8034e4113bae463657b99eced"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Read dataset of 20Newsgroup\n",
    "\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "df=pd.DataFrame({\"content\":newsgroups[\"data\"]})\n",
    "\n",
    "df[\"content\"]=df[\"content\"].swifter.apply(lambda x: preprocess_text(x))\n",
    "df['content_length'] = df['content'].str.len()\n",
    "\n",
    "df = df[df['content_length'] > 100]\n",
    "df = df[df['content_length'] < 2000]\n",
    "\n",
    "df=df[[\"content\"]].reset_index(drop=True).reset_index().rename(columns={\"index\":\"id\"})\n",
    "documents_20newsgroup=df.content.to_list()\n",
    "tokenized_docs = [word_tokenize(document.lower()) for document in documents_20newsgroup]\n",
    "model = gensim.models.Word2Vec(tokenized_docs, vector_size=300, window=5, min_count=1, workers=12)\n",
    "wv_20newsgroup=model.wv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "def word_embedding_coherence(topics,wv,topk=10):\n",
    "    result = 0.0\n",
    "    for topic in topics:\n",
    "        E = []\n",
    "        for word in topic[:topk]:\n",
    "            if word in wv:\n",
    "                try:\n",
    "                    word_embedding = wv[word].numpy()\n",
    "                except:\n",
    "                    word_embedding = wv[word]\n",
    "                normalized_we = word_embedding / word_embedding.sum()\n",
    "                E.append(normalized_we)\n",
    "\n",
    "        if len(E) > 0:\n",
    "            E = np.array(E)\n",
    "            # Perform cosine similarity between E rows\n",
    "            distances = np.sum(1-pairwise_distances(E, metric='cosine') - np.diag(np.ones(len(E))))\n",
    "            topic_coherence = distances/(topk*(topk-1))\n",
    "        else:\n",
    "            topic_coherence = -1\n",
    "            # Update result with the computed coherence of the topic\n",
    "        result += topic_coherence\n",
    "    result = result/len(topics)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "ntm_results_path = \"./NTM/results/\"\n",
    "coherence_result = \"coherence_result4.txt\"\n",
    "\n",
    "with open(coherence_result, \"w\") as f:\n",
    "    for documents in [(documents_EM_tweets,\"tweets\",wv_EM_tweets),(documents_20newsgroup,\"20newsgroup\",wv_20newsgroup)]:\n",
    "        documents_tokens=[doc.split() for doc in documents[0]]\n",
    "        dictionary = Dictionary(documents_tokens)\n",
    "        corpus_bow = [dictionary.doc2bow(doc_tokens) for doc_tokens in documents_tokens]\n",
    "        for filename in os.listdir(ntm_results_path):\n",
    "            name=filename.split(\"_\")\n",
    "            if 'topics' in name and documents[1] in name:\n",
    "                with open(\"NTM/results/\"+str(filename), \"rb\") as fp:\n",
    "                    topics = pickle.load(fp)\n",
    "                for i in [\"u_mass\",\"c_v\",\"c_uci\",\"c_npmi\"]:\n",
    "                    cm = CoherenceModel(topics=topics, corpus=corpus_bow, dictionary=dictionary, texts=documents_tokens, coherence=i)\n",
    "                    coherence=cm.get_coherence()\n",
    "                    f.write(f\"{filename}, {i}, {coherence}\\n\")\n",
    "                f.write(f\"{filename}, word_embedding_coherence, {word_embedding_coherence(topics,documents[2],topk=10)}\\n\")\n",
    "    f.write(\"The experiment is successfully done!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
