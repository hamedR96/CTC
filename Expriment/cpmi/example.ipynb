{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "def count_remover(text,threshold=4):\n",
    "    if len(text.split())<threshold:\n",
    "        return pd.NaT\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "data=pd.read_json('tweets.json' ,lines=True)\n",
    "df=data[[\"Text\",\"CreatedAt\"]].rename(columns={\"Text\":\"content\",\"CreatedAt\":\"time\"})\n",
    "df['content'] = df['content'].str.replace(r'@\\w+', '')\n",
    "df['content'] = df['content'].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
    "df['content'] = df['content'].apply(lambda x: remove_punct(x))\n",
    "df['content'] = df['content'].apply(lambda x: count_remover(x))\n",
    "df=df.dropna()\n",
    "documents_EM_tweets=df.content.tolist()\n",
    "#documents_EMsTweets=[document for document in documents if len(document.split())>10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/18846 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a4ae302c8f84047a8bc9a76f656be55"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import swifter\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[{}0-9]'.format(string.punctuation), ' ', text)\n",
    "    text=re.sub(r'[^A-Za-z0-9 ]+', ' ', text)\n",
    "    text = word_tokenize(text)\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    text = [WordNetLemmatizer().lemmatize(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "df=pd.DataFrame({\"content\":newsgroups[\"data\"]})\n",
    "\n",
    "df[\"content\"]=df[\"content\"].swifter.apply(lambda x: preprocess_text(x))\n",
    "df['content_length'] = df['content'].str.len()\n",
    "\n",
    "df = df[df['content_length'] > 100]\n",
    "df = df[df['content_length'] < 2000]\n",
    "\n",
    "df=df[[\"content\"]].reset_index(drop=True).reset_index().rename(columns={\"index\":\"id\"})\n",
    "documents_20newsgroup=df.content.to_list()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ntm_results_path = \"/Users/hamed/PycharmProjects/CTC/NTM/results/\"\n",
    "coherence_result = \"/Users/hamed/PycharmProjects/CTC/ctc_result.txt\"\n",
    "\n",
    "with open(coherence_result, \"w\") as f:\n",
    "    cpmi_result=[]\n",
    "    for documents in [(documents_EM_tweets,\"tweets\"),(documents_20newsgroup,\"20newsgroup\")]:\n",
    "        sentences=[sentence.strip().lower() for sentence in documents[0]]    #lowering the words in the dataset\n",
    "        sentences  = [\"\".join([char for char in text if char not in string.punctuation]) for text in sentences] #remove punctuation\n",
    "        sentences = [sentence.split(' ') for sentence in sentences] #tokenizing the dataset\n",
    "        sentences=[[token for token in sentence if len(token)>0] for sentence in sentences]\n",
    "        sentences=[sentence for sentence in sentences if len(sentence) > 5] #filtering the dataset\n",
    "        sentences = [sentence[i:i+20] for sentence in sentences for i in range(0, len(sentence), 20) ]\n",
    "        sentences=[sentence for sentence in sentences if len(sentence) > 5]\n",
    "\n",
    "        model_counter=0\n",
    "        for topic_model in os.listdir(ntm_results_path):\n",
    "            if documents[1] in topic_model.split(\"_\") and 'topics' in topic_model.split(\"_\"):\n",
    "                model_counter+=1\n",
    "\n",
    "        cpmi_matrix=np.zeros((len(sentences),model_counter))\n",
    "        c=1\n",
    "        for i,sentence in enumerate(tqdm(sentences)):\n",
    "\n",
    "\n",
    "            if len(sentence)>5:\n",
    "                try:\n",
    "\n",
    "                    outs = txt_to_pmi.get_cpmi(MODEL, [sentence], verbose=False)\n",
    "                    for j,filename in enumerate(os.listdir(ntm_results_path)):\n",
    "                        sentence_topic_score=0\n",
    "                        name=filename.split(\"_\")\n",
    "                        if ('topics' in name) and (documents[1] in name):\n",
    "                            with open(\"/Users/hamed/PycharmProjects/CTC/NTM/results/\"+str(filename), \"rb\") as fp:\n",
    "                                topics = pickle.load(fp)\n",
    "                            for topic in topics:\n",
    "                                topic_score=0\n",
    "                                words=list(set(topic) & set(sentence))\n",
    "                                if len(words)>1 :\n",
    "                                    for word1 in words:\n",
    "                                        w1=sentence.index(word1)\n",
    "                                        for word2 in words:\n",
    "                                            w2=sentence.index(word2)\n",
    "                                            topic_score+=outs[0][\"0\"][w1][w2]/2\n",
    "                                sentence_topic_score+=topic_score/len(topic)\n",
    "                        cpmi_matrix[i][j]=sentence_topic_score/len(topics)\n",
    "                        c+=1\n",
    "                except:\n",
    "\n",
    "                    continue\n",
    "        for j,filename in enumerate(os.listdir(ntm_results_path)):\n",
    "            f.write(f\"{filename}, {documents[1]} ,ctc, {np.sum(cpmi_matrix[:][j])}\\n\")\n",
    "        cpmi_result.append(cpmi_matrix)\n",
    "    f.write(\"The experiment is successfully done!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def cpmi_coherence(topics,documents,batch_size=50,min_batch_lenghth=5):\n",
    "    sentences=[sentence.strip().lower() for sentence in documents]    #lowering the words in the dataset\n",
    "    sentences  = [\"\".join([char for char in text if char not in string.punctuation]) for text in sentences] #remove punctuation\n",
    "    sentences = [sentence.split(' ') for sentence in sentences] #tokenizing the dataset\n",
    "    sentences=[[token for token in sentence if len(token)>0] for sentence in sentences]\n",
    "    sentences=[sentence for sentence in sentences if len(sentence) > min_batch_lenghth] #filtering the dataset\n",
    "    #batching the dataset\n",
    "    print(len(sentences))\n",
    "    sentences = [sentence[i:i+batch_size] for sentence in sentences for i in range(0, len(sentence), batch_size) ]\n",
    "    print(len(sentences))\n",
    "    #calculating CPMI for each batch\n",
    "    sentence_counter=0\n",
    "    coherence=0\n",
    "    for i,sentence in enumerate(tqdm(sentences)):\n",
    "        sentence_score=0\n",
    "        try:\n",
    "            outs = txt_to_pmi.get_cpmi(MODEL, [sentence], verbose=False)\n",
    "            sentence_counter+=1\n",
    "            for topic in topics:\n",
    "                topic_score=0\n",
    "                words=list(set(topic) & set(sentence))\n",
    "                if len(words)>1 :\n",
    "                    for word1 in words:\n",
    "                        w1=sentence.index(word1)\n",
    "                        for word2 in words:\n",
    "                            w2=sentence.index(word2)\n",
    "                            topic_score+=outs[0][\"0\"][w1][w2]/2\n",
    "                sentence_score+=topic_score/len(topic)\n",
    "        except:\n",
    "            print(\"ERORR in CPMI\")\n",
    "            continue\n",
    "        coherence+=sentence_score\n",
    "\n",
    "    return coherence/sentence_counter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import txt_to_pmi\n",
    "DEVICE = torch.device('mps')\n",
    "MODEL = txt_to_pmi.languagemodel.BERT(DEVICE, 'bert-base-cased', 32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/53363 [00:17<37:27:24,  2.53s/it]"
     ]
    }
   ],
   "source": [
    "sentences=[sentence.strip().lower() for sentence in documents_20newsgroup]\n",
    "sentences  = [\"\".join([char for char in text if char not in string.punctuation]) for text in sentences]\n",
    "sentences = [sentence.split(' ') for sentence in sentences]\n",
    "sentences=[[token for token in sentence if len(token)>0] for sentence in sentences]\n",
    "sentences=[sentence for sentence in sentences if len(sentence) > 5]\n",
    "sentences = [sentence[i :i+20] for sentence in sentences for i in range(0, len(sentence), 20) ]\n",
    "sentences=[sentence for sentence in sentences if len(sentence) > 5]\n",
    "outs = txt_to_pmi.get_cpmi(MODEL, sentences, verbose=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "with open(\"sentences_20\", \"wb\") as fp:   #Pickling\n",
    "  pickle.dump(sentences, fp)\n",
    "\n",
    "with open(\"out_20\", \"wb\") as fp:   #Pickling\n",
    "  pickle.dump(outs, fp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14624\n",
      "74056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 8891/74056 [2:24:36<18:55:06,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERORR in CPMI\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "with open(\"/Users/hamed/PycharmProjects/CTC/NTM/results/etm_20newsgroup_20_topics\", \"rb\") as fp:\n",
    "    topics = pickle.load(fp)\n",
    "cpmi_coherence(topics,documents_20newsgroup,batch_size=15,min_batch_lenghth=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pytorch",
   "language": "python",
   "display_name": "Python 3.9 (pytorch)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
